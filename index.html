<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sri Harsha Mudumba | ML Engineer</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Sri Harsha Mudumba</h1>
        <p style="text-align: center; font-size: 20px; font-weight: bold;">Aspiring Machine Learning Engineer</p>
        <p style="text-align: center;">HPC | AI Researcher</p>
        <nav>
            <ul>
                <li><a href="#about">About</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#paper-reviews">Reviews</a></li>
                <li><a href="#contact">Contact</a></li>
                <li><a href="#resume">Resume</a></li>
            </ul>
        </nav>
    </header>

    <section id="about" class="section">
        <h2>About Me</h2>
        <p>I am a Computer Engineer with a Masterâ€™s focus in <strong>Machine Learning</strong>, <strong>High-Performance Computing (HPC)</strong>, and <strong>Computer Architecture Systems</strong>, driven by a mission to make intelligent systems faster, more scalable, and production-ready.</p>
        <p>Through my graduate work at Iowa State University, Iâ€™ve built a strong foundation across model design, optimization, and deployment. I work at the intersection of AI development and systems engineering to deliver real-world solutions that balance speed, accuracy, and efficiency.</p>
        <p>Previously, I served as an Associate DBA at Cognizant, managing Oracle EBS applications for enterprise clients like Intuit and BioMarin â€” streamlining patching, automation, and performance tuning in production-scale environments.</p>
        <p>I am passionate about bridging AI research with real-world systems deployment and excited to apply my expertise in high-performance, scalable ML engineering.</p>
    </section>

    <section id="experience" class="section">
        <h2>Professional Experience</h2>
        <h3>Associate Database Administrator â€“ Cognizant Technology Solutions</h3>
        <p><strong>Clients:</strong> Intuit, BioMarin</p>
        <ul>
            <li>Maintained Oracle E-Business Suite (11g/12c/19c) environments and optimized mission-critical financial and biomedical systems.</li>
            <li>Performed ADOP patching, database upgrades, and environment cloning for seamless deployments.</li>
            <li>Developed shell scripts to automate patching, alerting, and environment refresh tasks.</li>
            <li>Integrated Active Directory for secure authentication and enforced audit logging policies.</li>
        </ul>
    </section>

    <section id="skills" class="section">
        <h2>Technical Skills</h2>
        <ul>
            <li><strong>Languages:</strong> Python, C++, Shell, SQL</li>
            <li><strong>Frameworks:</strong> PyTorch, TensorFlow, ONNX, EasyOCR, OpenCV</li>
            <li><strong>Tooling:</strong> HuggingFace, FastAPI, Git, Slurm, Oracle EBS</li>
            <li><strong>Domains:</strong> Model Compression, Video Processing, Generative AI, Smart Contracts</li>
            <li><strong>Infra:</strong> Multi-GPU training, HPC clusters, compiler stacks (ONNX/IREE)</li>
        </ul>
    </section>

    <section id="projects" class="section">
        <h2>Projects</h2>
        <div class="project-list">
            <div class="project-card">
                <h3><a href="#lazy-nn">Lazy Neural Network Optimization</a></h3>
                <p>Adaptive early exit mechanism to reduce inference cost using TensorFlow & PyTorch.</p>
            </div>
            <div class="project-card">
                <h3><a href="#1bitllm">1BitLLM Model Benchmarking</a></h3>
                <p>Analyzed quantized LLM performance using ONNX & IREE for low-latency deployment.</p>
            </div>
            <div class="project-card">
                <h3><a href="#snapea">Snapea - Efficient Inference</a></h3>
                <p>Confidence-based skipping and pruning to accelerate CNNs on edge devices.</p>
            </div>
            <div class="project-card">
                <h3><a href="#simnet">SimNet - Architecture Simulation</a></h3>
                <p>Deep learning model to predict microarchitectural behavior and improve simulation throughput.</p>
            </div>
            <div class="project-card">
                <h3><a href="#music-genre">Music Genre Classification</a></h3>
                <p>Used MFCC features and ML models to classify songs by genre in a web interface.</p>
            </div>
            <div class="project-card">
                <h3><a href="#anpr">Automatic Number Plate Recognition</a></h3>
                <p>YOLOv5 + EasyOCR-based ANPR system deployed for traffic and toll monitoring.</p>
            </div>
            <div class="project-card">
                <h3><a href="#ethereum-flight-insurance">Flight Insurance on Blockchain</a></h3>
                <p>Decentralized Ethereum contract to auto-process claims using real-time flight data.</p>
            </div>
        </div>
    </section>

    <section id="paper-reviews" class="section">
        <div class="container">
            <h2 class="section-title">ðŸ“„ Recent Paper Reviews</h2>

            <div class="review-entry">
                <h3>1. Memory is the New Compute: Overview of PIM Circuits for AI/ML</h3>
                <p><strong>Summary:</strong> Based on IEEE JETCAS 2022, this review explores how SRAM, DRAM, and ReRAM-based Processing-in-Memory (PIM) circuits are transforming ML workloads by reducing data movement overhead. The paper discusses hardware-software co-design, runtime scheduling, and offloading strategies.</p>
                <p><strong>Connection to Thesis:</strong> My thesis investigates Heterogeneous PIM with Early Exits â€” combining analog/digital memory compute with entropy-based dynamic inference for energy-efficient ML.</p>
                <p><strong>Key Takeaways:</strong> Dynamic resource allocation, lower energy per inference, smarter scheduling. Compute is not just moving closer to memory â€” it's becoming smarter.</p>
            </div>

            <div class="review-entry">
                <h3>2. Early Exit Optimization in Deep Networks for Heterogeneous PIM Architectures</h3>
                <p><strong>Overview:</strong> This review summarizes my thesis work on entropy-driven early exits in deep neural networks. Using ResNet50 as a base, I trained 3-exit and 4-exit models to allow confident predictions at intermediate layers, significantly reducing inference latency.</p>
                <p><strong>Results:</strong> At lower entropy thresholds, early exits achieved 80.55% accuracy (3-exit) and 75.78% (4-exit), proving this architectureâ€™s value in deployment-centric scenarios.</p>
                <p><strong>Contribution:</strong> Dynamic inference, joint training of multiple exits, and hardware-aligned energy-efficient model design.</p>
            </div>

            <div class="review-entry">
                <h3>3. Revolutionizing LLM Inference with NeuPIMs</h3>
                <p><strong>Insight:</strong> This paper introduces NeuPIMs â€” a heterogeneous system combining NPUs and PIM to optimize matrix-heavy LLM inference. NeuPIMs exploit NPU strength in GEMMs and PIM's advantage in GEMVs through sub-batch interleaving and runtime workload distribution.</p>
                <p><strong>Why It Matters:</strong> With up to 3Ã— throughput improvements for LLM inference, NeuPIMs address the growing bottleneck in AI hardware by enabling parallel compute and memory execution.</p>
                <p><strong>My View:</strong> As LLMs scale, PIM-based accelerators like NeuPIMs will be crucial for unlocking real-time, power-efficient inference.</p>
            </div>
        </div>
    </section>

    <section id="contact" class="section">
        <h2>Contact</h2>
        <p><strong>Email:</strong> <a href="mailto:srim@iastate.edu">srim@iastate.edu</a></p>
        <p><strong>Phone:</strong> +1 (515)-916-3011</p>
        <p><strong>LinkedIn:</strong> <a href="https://linkedin.com/in/sriharshamudumba" target="_blank">sriharshamudumba</a></p>
        <p><strong>GitHub:</strong> <a href="https://github.com/sriharshamudumba" target="_blank">sriharshamudumba</a></p>
    </section>

    <section id="resume" class="section">
        <h2>Resume</h2>
        <p>Download my latest resume:
            <a href="Sriharsha_Mudumba_resume_apple.pdf" target="_blank" style="font-weight: bold; color: #a478e0;">
                Click Here
            </a>
        </p>
    </section>

    <footer>
        <p>&copy; 2024 Sri Harsha Mudumba. All rights reserved.</p>
    </footer>
</body>
</html>
